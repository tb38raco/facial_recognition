{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#       Eine kurze Anleitung, um Gesichtserkennungs und -verifikation selbst ausprobieren zu können\n",
    "##0     Verwendete Module\n",
    "\n",
    "        DeepFace: DeepFace ist eine Bibliothek für Python, die sich auf Gesichtsverarbeitung und -erkennung spezialisiert hat. Sie ermöglicht die Arbeit mit Gesichtsmerkmalen, Gesichtsverifikation (Überprüfung, ob zwei Gesichter derselben Person gehören) und Emotionserkennung in Bildern.\n",
    "\n",
    "        OpenCV (cv2): OpenCV ist eine Open-Source-Bibliothek für Computer Vision. Sie wird verwendet, um Bilder und Videos zu verarbeiten, Gesichtserkennung durchzuführen und visuelle Aufgaben in Python zu automatisieren.\n",
    "\n",
    "    FER (Facial Expression Recognition): FER ist eine Bibliothek für die Erkennung von Gesichtsausdrücken. Sie hilft dabei, Emotionen in Gesichtern zu erkennen, wie zum Beispiel Lächeln, Traurigkeit oder Überraschung.\n",
    "\n",
    "    Tabulate: Die tabulate-Bibliothek ist nützlich, um Tabellen mit Daten zu erstellen und sie in einer übersichtlichen Form darzustellen. Im gegebenen Code wird sie verwendet, um Ergebnisse in Tabellenform auszugeben.\n",
    "\n",
    "Die Bibliotheken lassen sich via 'pip install *Bibliothekname*' installieren"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "from fer import FER\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#1      Automatisierte Gesichtserkennung und -verifikation\n",
    "\n",
    "##1.1   Vergleichsbilder\n",
    "\n",
    "        Zuerst wählen wir einen Ordner aus, der Bilder mit Gesichtern enthählt. Diser sollte sich im selben Verzeichnis wie unser Code befinden"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Pfad zum Ordner mit den Bildern\n",
    "folder_path = 'IMG_1001.JPG'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##1.2   Gesichtserkennung und Merkmalsvektor\n",
    "\n",
    "        In diesem Code werden viele Schritte durchgeführt, um Gesichter in Bildern zu erkennen und dann festzustellen, wie ähnlich oder verschieden diese Gesichter sind.\n",
    "        Mit der for-Schleife betrachten wir jedes einzelne Bild im Ordner.\n",
    "        Dann nehmen wir zwei Bilder und erstellen ein \"Bildpaar\" aus diesen beiden Bildern, um zu überprüfen, wie ähnlich oder verschieden sie sind.\n",
    "        Erkenne Gesichter: Wir schauen uns jedes Bild in diesem Bildpaar an und versuchen, Gesichter darauf zu finden.\n",
    "        Durchführen der Gesichtsverifikation: Wenn wir in beiden Bildern Gesichter finden, führen wir einen speziellen Prozess namens \"Gesichtsverifikation\" durch. Dieser Prozess untersucht,\n",
    "        wie ähnlich oder verschieden die Gesichter in den Bildern sind. Wir verwenden ein Modell namens \"VGG-Face\" für diese Überprüfung.\n",
    "        [VGG-Face](https://exposing.ai/vgg_face/) ist ein tiefes neuronales Netzwerkmodell für die Gesichtserkennung und die Extraktion von Gesichtsmerkmalen. Eine der Hauptaufgaben von VGG-Face ist die Extraktion von Merkmalen aus Gesichtern.\n",
    "        Dazu wird jedes Bild zunächst auf eine einheitliche Pixel-Größe zugeschnitten und aus jedem Gesicht einen sogenannten Merkmalsvektor erzeugt. Diese übersetzung eines Bildes in eine für Maschinen erfassbare Sprache, also die der Mathematik, bildet die Grundlage für alle weiteren Aufgaben, die unser Gesichtserkennungssystem erfüllen soll.\n",
    "        Die Deepface-Bibliothek erlaubt die Auswahl unterschiedlichster Modell und Metriken. So können wir beispielsweise in Zeile 18 'VGG-Face' durch 'OpenFace' ersetzen. Genauere Informationen über verfügbare Alternativen findet Ihr über diesen [link] (https://github.com/serengil/deepface/blob/master/deepface/DeepFace.py)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[link] (https://github.com/serengil/deepface/blob/master/deepface/DeepFace.py)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Liste, um die Ergebnisse der Gesichtsverifikation zu speichern\n",
    "results = []\n",
    "\n",
    "# Durchlaufe alle Bilder im Ordner\n",
    "files = os.listdir(folder_path)\n",
    "for i in range(len(files)):\n",
    "    for j in range(i + 1, len(files)):\n",
    "        # Pfade zu den Bildern erstellen\n",
    "        img1_path = os.path.join(folder_path, files[i])\n",
    "        img2_path = os.path.join(folder_path, files[j])\n",
    "\n",
    "        # Gesichter in den Bildern erkennen (mit DeepFace)\n",
    "        detected_faces_img1 = DeepFace.extract_faces(img1_path, enforce_detection=False)\n",
    "        detected_faces_img2 = DeepFace.extract_faces(img2_path, enforce_detection=False)\n",
    "\n",
    "        if detected_faces_img1 is not None and detected_faces_img2 is not None:\n",
    "            # Verifikationsprozess durchführen\n",
    "            result = DeepFace.verify(img1_path=img1_path, img2_path=img2_path, model_name='VGG-Face', enforce_detection=False)\n",
    "\n",
    "            # Ergebnis zur Ergebnisliste hinzufügen (als Liste, nicht als Tupel)\n",
    "            results.append([files[i], files[j], result[\"distance\"], result[\"verified\"]])\n",
    "        else:\n",
    "            # Wenn Gesichter nicht erkannt wurden, geben Sie den Bildtitel aus\n",
    "            print(f\"Gesichter in {files[i]} und {files[j]} wurden nicht erkannt.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##1.3 Formatierung der Ergebnisse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Erstellen Sie eine Tabelle mit den Ergebnissen\n",
    "table_headers = [\"Bild 1\", \"Bild 2\", \"Distanz\", \"Wahrheitswert\"]\n",
    "table_data = results\n",
    "\n",
    "# Tabelle anzeigen\n",
    "print(tabulate(table_data, headers=table_headers, tablefmt=\"pretty\"))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##1.4   Gesichtsverifikation mit Schwellenwert\n",
    "\n",
    "        Wir erinnern uns, das jedes Gesicht nun als Vektor vorliegt, wobei die einzelnen Koordinaten die n Gesichtsmerkmale\n",
    "        im n-dimensionalen Merkmalsraum darstellen. Diese kann die KI nun verwenden um Gesichter miteinander zu vergleichen.\n",
    "        Dazu benẗigen wir eine Vegleichsmetrik, auch als Distanz bezeichnet, die wir hier verwenden, ist die Kosinus Ähnlichkeit,\n",
    "        also der Winkel zwischen zwei Vektoren. Diese berechnet sich durch $cosine_similarity(A,B) = 1-cos(A,B)$ Das bedeutet die\n",
    "        Distanz kann Werte zwischen 0 und 1 annehmen und je näher der Wert an 0 liegt, desto ähnlicher sind sich zwei Gesichter.\n",
    "        An dieser Stelle kommt der Schwellenwert (threshold) ins Spiel. Ob die die Gesichtsverifikation zweier Gesichter als wahr\n",
    "        oder falsch ausgewertet wird, ist abhängig von dem Wert den wir hier festlegen. Standartmäßig lag dieser im vorangegangenen\n",
    "        Codeblock bei 0.4."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Festlegen des Schwellenwerts für die Gesichtsverifikation\n",
    "threshold = 0.6  # Ändern Sie diesen Schwellenwert nach Bedarf\n",
    "\n",
    "# Überprüfen Sie, ob der Wahrheitswert basierend auf dem Schwellenwert True oder False ist\n",
    "updated_table_data = []\n",
    "for row in table_data:\n",
    "    distance = row[2]\n",
    "    is_verified = distance < threshold\n",
    "    updated_table_data.append([row[0], row[1], distance, is_verified])\n",
    "\n",
    "# Aktualisierte Tabelle anzeigen\n",
    "print(\"\\nAktualisierte Tabelle mit dem Schwellenwert:\")\n",
    "print(tabulate(updated_table_data, headers=table_headers, tablefmt=\"pretty\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#2      Verwendung der live-Eingabe (hierfür benötigt ihr eine Webcam)\n",
    "\n",
    "##2.1   Gesichtsekennung, emotion- und age detection.\n",
    "\n",
    "        Emotion-Recognizer-Objekt: Dies ist ein Objekt, das verwendet wird, um Emotionen in Gesichtern zu erkennen.\n",
    "        Es ist so eingerichtet, dass es in Bildern nach Gesichtern sucht und dann die Emotionen in diesen Gesichtern identifiziert.\n",
    "\n",
    "        Gesichtserkennungskaskade: Dies ist ein Hilfsmittel für die Gesichtserkennung. Die \"Kaskade\" ist eine spezielle Methode,\n",
    "        die in OpenCV (Computer Vision Library) verwendet wird, um Gesichter in Bildern zu finden. Diese \"Kaskade\" wurde bereits\n",
    "        trainiert, um die Merkmale von Gesichtern zu erkennen, wie Augen, Nase und Mund. Es hilft, die Positionen von Gesichtern\n",
    "        in Bildern zu finden. Der Unterschied zum Deep Learning Anzatz, der auf tiefen neuronalen Netzen beruht, verwendet Haar-cascade\n",
    "        statistische Algorithmen auf handgefertigten Merkmalen. Ein Vorteil liegt darin, dass diese Algorithmen schneller berechnet\n",
    "        werden können und daher oft in Live-Anwendungen eingesetzt werden. Die geringere Genauigkeit im Vergleich zu Deep Learning\n",
    "        Modellen nehmen wir für universellere Anwendung in Kauf."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Emotion-Recognizer-Objekt erstellen\n",
    "emotion_recognizer = FER()\n",
    "\n",
    "# Gesichtserkennungskaskade laden\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Flag für die Anzeige der Emotionen und Altersschätzung\n",
    "show_emotions = True\n",
    "show_age = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##2.2 Kameraverwendung\n",
    "\n",
    "    Dieser Code erstellt also einen Echtzeit-Video-Stream, auf dem Gesichter erkannt und optional Emotionen und Altersschätzungen angezeigt werden.\n",
    "    Zuerst wird eine Endlosschleife erstellt, die kontinuierlich Bilder vom Kamerastream liest. Für jedes gelesene Bild wird die Gesichtserkennung\n",
    "    durchgeführt, um Gesichter im Bild zu identifizieren. Wenn Gesichter erkannt werden:\n",
    "\n",
    "    Wenn die Variable show_emotions aktiviert ist, werden Emotionen in den erkannten Gesichtern identifiziert und auf dem Bild angezeigt.\n",
    "\n",
    "    Wenn die Variable show_age aktiviert ist, wird das Alter der erkannten Gesichter geschätzt und auf dem Bild angezeigt.\n",
    "\n",
    "    Andernfalls wird nur ein grüner Rahmen um die erkannten Gesichter gezeichnet.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Kamera initialisieren\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Bild vom Kamerastream lesen\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    # Gesichtserkennung im Bild durchführen\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        if show_emotions:\n",
    "            # Gesichtsausschnitt extrahieren\n",
    "            face_image = frame[y:y + h, x:x + w]\n",
    "\n",
    "            # Emotionen im Gesicht erkennen\n",
    "            result = emotion_recognizer.detect_emotions(face_image)\n",
    "\n",
    "            if len(result) > 0:\n",
    "                # Emotionen auf dem Bild anzeigen\n",
    "                emotions = result[0]['emotions']\n",
    "                dominant_emotion = max(emotions, key=emotions.get)\n",
    "                cv2.putText(frame, dominant_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        if show_age:\n",
    "            # Alter schätzen\n",
    "            age = int(w / 10)  # Einfache Schätzung basierend auf Gesichtsbreite\n",
    "            cv2.putText(frame, f\"AGE: {age}\", (x, y + h + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        else:\n",
    "            # Nur den grünen Kasten anzeigen\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##2.3 Livestream anzeigen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "    cv2.imshow('Emotion and Age Detection', frame)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##2.4 Shortcuts\n",
    "\n",
    "        Um eine komfortablere Anwendung zu ermöglichen, haben wir Tastaturkurzbefehle integriert, damit wir wählen können\n",
    "        welche Ergebnisse angezeigt werden und die Anwendung geschlossen werden kann."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "    # Tastenabfrage\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or key == ord('Q'):  # 'q' drücken, um die Schleife zu beenden\n",
    "        break\n",
    "    elif key == ord('e') or key == ord('E'):  # 'e' drücken, um die Emotionen ein- oder auszublenden\n",
    "        show_emotions = not show_emotions\n",
    "    elif key == ord('a') or key == ord('A'):  # 'a' drücken, um das Alter ein- oder auszublenden\n",
    "        show_age = not show_age"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##2.5 Kameraressourcen freigeben\n",
    "\n",
    "        Um eurer Maschine mitzuteilen, dass die Berechnung beendet ist, ist es immer wichtig, dass wir die gestarteten Prozesse sauber beenden."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "        Wir hoffen, dass ihr viel Freude mit dieser Anleitung habt und Interesse an den Themen Deep Learning und Gesichtserkennung gefunden habt.\n",
    "\n",
    "        PS: Falls ihr noch mehr ausprobieren und wissen wollt, schaut [hier]() mal rein."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
