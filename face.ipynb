{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#       __Kurzanleitung Gesichtserkennung & -verifikation__\n",
    "## 0    Verwendete Module\n",
    "\n",
    "__DeepFace__: DeepFace ist eine Bibliothek für Python, die sich auf Gesichtsverarbeitung und \n",
    "-erkennung spezialisiert hat. Sie ermöglicht die Arbeit mit Gesichtsmerkmalen, Gesichtsverifikation\n",
    "(Überprüfung, ob zwei Gesichter derselben Person gehören) und Emotionserkennung in Bildern.\n",
    "\n",
    "__OpenCV__ (cv2): OpenCV ist eine Open-Source-Bibliothek für Computer Vision. Diese wird verwendet, \n",
    "um Bilder und Videos zu verarbeiten, Gesichtserkennung durchzuführen und visuelle Aufgaben in \n",
    "Python zu automatisieren.\n",
    "\n",
    "__FER__ (Facial Expression Recognition): FER ist eine Bibliothek für die Erkennung von Gesichtsausdrücken.\n",
    "Sie hilft dabei, Emotionen innerhalb von Gesichtern zu erkennen, wie zum Beispiel glücklich oder traurig.\n",
    "\n",
    "__Tabulate__: Die tabulate-Bibliothek ist nützlich, um Tabellen mit Daten zu erstellen und sie in einer \n",
    "übersichtlichen Form darzustellen. Im gegebenen Code wird sie verwendet, um Ergebnisse in Tabellenform\n",
    "auszugeben.\n",
    "\n",
    "Die Bibliotheken lassen sich via 'pip install *Bibliothekname*' installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "from fer import FER\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1      Automatisierte Gesichtserkennung und -verifikation\n",
    "\n",
    "### 1.1   Vergleichsbilder\n",
    "\n",
    ">Zuerst wählen wir einen Ordner aus, der Bilder mit Gesichtern enthählt. Diser sollte sich im selben \n",
    ">Verzeichnis wie unser Code befinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T18:30:52.627057581Z",
     "start_time": "2023-09-06T18:30:52.614592077Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pfad zum Ordner mit den Bildern\n",
    "folder_path = 'IMG_1001.JPG'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Erkennungs- & Verifikationsprozess  \n",
    "In diesem Code werden viele Schritte durchgeführt, um Gesichter in Bildern zu erkennen und dann festzustellen, wie ähnlich oder verschieden diese Gesichter sind.\n",
    "Mit der for-Schleife betrachten wir jedes einzelne Bild im Ordner.\n",
    "        Dann nehmen wir zwei Bilder und erstellen ein \"Bildpaar\" aus diesen beiden Bildern, um zu überprüfen, wie ähnlich oder verschieden sie sind.\n",
    "        Erkenne Gesichter: Wir schauen uns jedes Bild in diesem Bildpaar an und versuchen, Gesichter darauf zu finden.\n",
    "        Durchführen der Gesichtsverifikation: Wenn wir in beiden Bildern Gesichter finden, führen wir eine *Gesichtsverifikation* durch.\n",
    "        Für diese Überprüfung verwenden ein Modell namens \"VGG-Face\".\n",
    "        [VGG-Face](https://exposing.ai/vgg_face/) ist ein tiefes [neuronales Netzwerkmodell](https://www.heise.de/ratgeber/Neuronale-Netz-einfach-erklaert-6343697.html), dessen Hauptaufgabe in der Gesichtserkennung und der Extraktion von Gesichtsmerkmalen besteht.\n",
    "        Dazu wird jedes Bild zunächst durch Vorverarbeitung auf ein einheitliches Format gebracht und anschließend aus jedem erkannten Gesicht ein sogenannter *Merkmalsvektor* erzeugt. Diese Übersetzung eines Bildes in eine für Maschinen erfassbare Sprache, also die der Mathematik, bildet die Grundlage für alle weiteren Aufgaben, die unser Gesichtserkennungssystem erfüllen soll. Diese Vorgehensweise ist typisch für Deep Learning-Bildverarbeitungsmodelle.\n",
    "        Die Deepface-Bibliothek erlaubt außerdem eine Auswahl unterschiedlichster Modell und Metriken. So können wir beispielsweise in Zeile 18 'VGG-Face' durch 'OpenFace' ersetzen und die Ergebnisse in Abhängigkeit des verwendeten Modells beobachten. Genauere Informationen über verfügbare Alternativen findet ihr [hier](https://github.com/serengil/deepface/blob/master/deepface/DeepFace.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Liste, um die Ergebnisse der Gesichtsverifikation zu speichern\n",
    "results = []\n",
    "\n",
    "# Durchlaufe alle Bilder im Ordner\n",
    "files = os.listdir(folder_path)\n",
    "for i in range(len(files)):\n",
    "    for j in range(i + 1, len(files)):\n",
    "        # Pfade zu den Bildern erstellen\n",
    "        img1_path = os.path.join(folder_path, files[i])\n",
    "        img2_path = os.path.join(folder_path, files[j])\n",
    "\n",
    "        # Gesichter in den Bildern erkennen (mit DeepFace)\n",
    "        detected_faces_img1 = DeepFace.extract_faces(img1_path, enforce_detection=False)\n",
    "        detected_faces_img2 = DeepFace.extract_faces(img2_path, enforce_detection=False)\n",
    "\n",
    "        if detected_faces_img1 is not None and detected_faces_img2 is not None:\n",
    "            # Verifikationsprozess durchführen\n",
    "            result = DeepFace.verify(img1_path=img1_path, img2_path=img2_path, model_name='VGG-Face', enforce_detection=False)\n",
    "\n",
    "            # Ergebnis zur Ergebnisliste hinzufügen (als Liste, nicht als Tupel)\n",
    "            results.append([files[i], files[j], result[\"distance\"], result[\"verified\"]])\n",
    "        else:\n",
    "            # Wenn Gesichter nicht erkannt wurden, geben Sie den Bildtitel aus\n",
    "            print(f\"Gesichter in {files[i]} und {files[j]} wurden nicht erkannt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 1.3 Formatierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Erstellen Sie eine Tabelle mit den Ergebnissen\n",
    "table_headers = [\"Bild 1\", \"Bild 2\", \"Distanz\", \"Wahrheitswert\"]\n",
    "table_data = results\n",
    "\n",
    "# Tabelle anzeigen\n",
    "print(tabulate(table_data, headers=table_headers, tablefmt=\"pretty\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 1.4   Gesichtsverifikation mit Schwellenwert\n",
    "\n",
    "Wir erinnern uns, das jedes Gesicht nun als Vektor vorliegt, wobei die einzelnen Koordinaten die n-vielen Gesichtsmerkmale\n",
    "im n-dimensionalen Merkmalsraum darstellen. Diese kann die KI nun verwenden um Gesichter miteinander zu vergleichen.\n",
    "Dazu benötigen wir eine Vegleichsmetrik, auch als Distanz bezeichnet. Wir verwenden hier die Kosinus Ähnlichkeit,\n",
    "also der Winkel zwischen zwei Vektoren. Diese berechnet sich durch $\\text{cosine_similarity}(A, B) = 1 - \\cos(A, B)$, was ihr [hier](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d) nochmal genauer nachlesen könnt.\n",
    " Das bedeutet die\n",
    "Distanz kann Werte zwischen 0 und 1 annehmen und je näher der Wert an 0 liegt, desto ähnlicher sind sich zwei Gesichter.\n",
    "An dieser Stelle kommt der Schwellenwert (_threshold_) ins Spiel. Ob die die Gesichtsverifikation zweier Gesichter als wahr\n",
    "oder falsch ausgewertet wird, ist abhängig von dem Wert den wir hier festlegen. Standardmäßig lag dieser im vorangegangenen\n",
    "Codeblock bei 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Festlegen des Schwellenwerts für die Gesichtsverifikation\n",
    "threshold = 0.6  # Ändern Sie diesen Schwellenwert nach Bedarf\n",
    "\n",
    "# Überprüfen Sie, ob der Wahrheitswert basierend auf dem Schwellenwert True oder False ist\n",
    "updated_table_data = []\n",
    "for row in table_data:\n",
    "    distance = row[2]\n",
    "    is_verified = distance < threshold\n",
    "    updated_table_data.append([row[0], row[1], distance, is_verified])\n",
    "\n",
    "# Aktualisierte Tabelle anzeigen\n",
    "print(\"\\nAktualisierte Tabelle mit dem Schwellenwert:\")\n",
    "print(tabulate(updated_table_data, headers=table_headers, tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2      Verwendung der live-Eingabe (hierfür benötigt ihr eine Webcam)\n",
    "\n",
    "### 2.1   Gesichtsekennung, emotion- und age detection.\n",
    "\n",
    "Emotion-Recognizer-Objekt: Dies ist ein Objekt, das verwendet wird, um Emotionen in Gesichtern zu erkennen.\n",
    "Es ist so eingerichtet, dass es in Bildern nach Gesichtern sucht und dann die Emotionen in diesen Gesichtern \n",
    "identifiziert.\n",
    "\n",
    "Gesichtserkennungskaskade: Dies ist ein Hilfsmittel für die Gesichtserkennung. Die \"Kaskade\" ist eine spezielle \n",
    "Methode, die in OpenCV (_Computer Vision Library_) verwendet wird, um Gesichter in Bildern zu finden. Diese \"Kaskade\" \n",
    "wurde bereits trainiert, um die Merkmale von Gesichtern zu erkennen, wie Augen, Nase und Mund. Es hilft, die \n",
    "Positionen von Gesichtern in Bildern zu finden. Der Unterschied zum Deep Learning Anzatz, der auf tiefen neuronalen \n",
    "Netzen beruht, verwendet Haar-cascade statistische Algorithmen auf handgefertigten Merkmalen. Ein Vorteil liegt darin,\n",
    "dass diese Algorithmen schneller berechnet werden können und daher oft in Live-Anwendungen eingesetzt werden. Die \n",
    "geringere Genauigkeit im Vergleich zu Deep Learning Modellen nehmen wir für universellere Anwendbarkeit in Kauf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion-Recognizer-Objekt erstellen\n",
    "emotion_recognizer = FER()\n",
    "\n",
    "# Gesichtserkennungskaskade laden\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Flag für die Anzeige der Emotionen und Altersschätzung\n",
    "show_emotions = True\n",
    "show_age = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.2 Kameraverwendung\n",
    "\n",
    "Dieser Code erstellt also einen Echtzeit-Video-Stream, auf dem Gesichter erkannt und optional Emotionen und Altersschätzungen angezeigt werden.\n",
    "    \n",
    "Zuerst wird eine Endlosschleife erstellt, die kontinuierlich Bilder vom Kamerastream liest. Für jedes gelesene Bild wird \n",
    "die Gesichtserkennung durchgeführt, um Gesichter im Bild zu identifizieren. Wenn Gesichter erkannt werden:\n",
    "\n",
    "__Wenn__ die Variable show_emotions aktiviert ist, werden Emotionen in den erkannten Gesichtern identifiziert und auf dem Bild angezeigt.\n",
    "       \n",
    "__Wenn__ die Variable show_age aktiviert ist, wird das Alter der erkannten Gesichter geschätzt und auf dem Bild angezeigt.\n",
    "    \n",
    "__Andernfalls__ wird nur ein grüner Rahmen um die erkannten Gesichter gezeichnet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Kamera initialisieren\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m camera \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Bild vom Kamerastream lesen\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m camera\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Kamera initialisieren\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Bild vom Kamerastream lesen\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    # Gesichtserkennung im Bild durchführen\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        if show_emotions:\n",
    "            # Gesichtsausschnitt extrahieren\n",
    "            face_image = frame[y:y + h, x:x + w]\n",
    "\n",
    "            # Emotionen im Gesicht erkennen\n",
    "            result = emotion_recognizer.detect_emotions(face_image)\n",
    "\n",
    "            if len(result) > 0:\n",
    "                # Emotionen auf dem Bild anzeigen\n",
    "                emotions = result[0]['emotions']\n",
    "                dominant_emotion = max(emotions, key=emotions.get)\n",
    "                cv2.putText(frame, dominant_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        if show_age:\n",
    "            # Alter schätzen\n",
    "            age = int(w / 10)  # Einfache Schätzung basierend auf Gesichtsbreite\n",
    "            cv2.putText(frame, f\"AGE: {age}\", (x, y + h + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        else:\n",
    "            # Nur den grünen Kasten anzeigen\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.3 Livestream anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    cv2.imshow('Emotion and Age Detection', frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.4 Shortcuts\n",
    "\n",
    "Um eine komfortablere Anwendung zu ermöglichen, haben wir Tastaturkurzbefehle integriert, damit wir wählen können \n",
    "welche Ergebnisse angezeigt werden und die Anwendung geschlossen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Tastenabfrage\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or key == ord('Q'):  # 'q' drücken, um die Schleife zu beenden\n",
    "        break\n",
    "    elif key == ord('e') or key == ord('E'):  # 'e' drücken, um die Emotionen ein- oder auszublenden\n",
    "        show_emotions = not show_emotions\n",
    "    elif key == ord('a') or key == ord('A'):  # 'a' drücken, um das Alter ein- oder auszublenden\n",
    "        show_age = not show_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.5 Kameraressourcen freigeben\n",
    "\n",
    "    Um eurer Maschine mitzuteilen, dass die Berechnung beendet ist, ist es immer wichtig, dass wir die gestarteten Prozesse sauber beenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir hoffen, dass ihr viel Freude mit dieser Anleitung habt und Interesse an den Themen Deep Learning und Gesichtserkennung gefunden habt.\n",
    "\n",
    "PS: Falls ihr noch mehr ausprobieren und wissen wollt, schaut [hier](https://www.kaggle.com/search?q=face+detection) mal rein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
