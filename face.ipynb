{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "modelle/module erklären"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required: pip install deepface matplotlib\n",
    "import os\n",
    "\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from fer import FER\n",
    "from tabulate import tabulate  # Installieren Sie die tabulate-Bibliothek mit \"pip install tabulate\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Teil 1\n",
    "hier können eigene ordner eingefügt werden, die Bilder mit gesichtern enthalten"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Pfad zum Ordner mit den Bildern\n",
    "folder_path = 'IMG_1001.JPG'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In diesem Code werden viele Schritte durchgeführt, um Gesichter in Bildern zu erkennen und dann festzustellen, wie ähnlich oder verschieden diese Gesichter sind.\n",
    "\n",
    "Mit der for-Schleife betrachten wir jedes einzelne Bild im Ordner.\n",
    "Dann nehmen wir zwei Bilder und erstellen ein \"Bildpaar\" aus diesen beiden Bildern, um zu überprüfen, wie ähnlich oder verschieden sie sind.\n",
    "Erkenne Gesichter: Wir schauen uns jedes Bild in diesem Bildpaar an und versuchen, Gesichter darauf zu finden.\n",
    "Durchführen der Gesichtsverifikation: Wenn wir in beiden Bildern Gesichter finden, führen wir einen speziellen Prozess namens \"Gesichtsverifikation\" durch. Dieser Prozess untersucht,\n",
    "wie ähnlich oder verschieden die Gesichter in den Bildern sind. Wir verwenden ein Modell namens \"VGG-Face\" für diese Überprüfung.\n",
    "Im Detail passiert folgendes: hier noch erklärung was under the hood passiert, also vorverarbeitung durch vgg face, und repräsentation eines gesichts als vektors durch vgg face modell + darauf hinweisen welche anderen modelle man in deepface auswählen kann"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Liste, um die Ergebnisse der Gesichtsverifikation zu speichern\n",
    "results = []\n",
    "\n",
    "# Durchlaufe alle Bilder im Ordner\n",
    "files = os.listdir(folder_path)\n",
    "for i in range(len(files)):\n",
    "    for j in range(i + 1, len(files)):\n",
    "        # Pfade zu den Bildern erstellen\n",
    "        img1_path = os.path.join(folder_path, files[i])\n",
    "        img2_path = os.path.join(folder_path, files[j])\n",
    "\n",
    "        # Gesichter in den Bildern erkennen (mit DeepFace)\n",
    "        detected_faces_img1 = DeepFace.extract_faces(img1_path, enforce_detection=False)\n",
    "        detected_faces_img2 = DeepFace.extract_faces(img2_path, enforce_detection=False)\n",
    "\n",
    "        if detected_faces_img1 is not None and detected_faces_img2 is not None:\n",
    "            # Verifikationsprozess durchführen\n",
    "            result = DeepFace.verify(img1_path=img1_path, img2_path=img2_path, model_name='VGG-Face', enforce_detection=False)\n",
    "\n",
    "            # Ergebnis zur Ergebnisliste hinzufügen (als Liste, nicht als Tupel)\n",
    "            results.append([files[i], files[j], result[\"distance\"], result[\"verified\"]])\n",
    "        else:\n",
    "            # Wenn Gesichter nicht erkannt wurden, geben Sie den Bildtitel aus\n",
    "            print(f\"Gesichter in {files[i]} und {files[j]} wurden nicht erkannt.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Erstellen Sie eine Tabelle mit den Ergebnissen\n",
    "table_headers = [\"Bild 1\", \"Bild 2\", \"Distanz\", \"Wahrheitswert\"]\n",
    "table_data = results\n",
    "\n",
    "# Tabelle anzeigen\n",
    "print(tabulate(table_data, headers=table_headers, tablefmt=\"pretty\"))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "hier können wir den schwellenwert selbst festlegen und ergebnisse ausgeben lassen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Festlegen des Schwellenwerts für die Gesichtsverifikation\n",
    "threshold = 0.6  # Ändern Sie diesen Schwellenwert nach Bedarf\n",
    "\n",
    "# Überprüfen Sie, ob der Wahrheitswert basierend auf dem Schwellenwert True oder False ist\n",
    "updated_table_data = []\n",
    "for row in table_data:\n",
    "    distance = row[2]\n",
    "    is_verified = distance < threshold\n",
    "    updated_table_data.append([row[0], row[1], distance, is_verified])\n",
    "\n",
    "# Aktualisierte Tabelle anzeigen\n",
    "print(\"\\nAktualisierte Tabelle mit dem Schwellenwert:\")\n",
    "print(tabulate(updated_table_data, headers=table_headers, tablefmt=\"pretty\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Teil 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Emotion-Recognizer-Objekt erstellen\n",
    "emotion_recognizer = FER()\n",
    "\n",
    "# Gesichtserkennungskaskade laden\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Flag für die Anzeige der Emotionen und Altersschätzung\n",
    "show_emotions = True\n",
    "show_age = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Kamera initialisieren\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Bild vom Kamerastream lesen\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    # Gesichtserkennung im Bild durchführen\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        if show_emotions:\n",
    "            # Gesichtsausschnitt extrahieren\n",
    "            face_image = frame[y:y + h, x:x + w]\n",
    "\n",
    "            # Emotionen im Gesicht erkennen\n",
    "            result = emotion_recognizer.detect_emotions(face_image)\n",
    "\n",
    "            if len(result) > 0:\n",
    "                # Emotionen auf dem Bild anzeigen\n",
    "                emotions = result[0]['emotions']\n",
    "                dominant_emotion = max(emotions, key=emotions.get)\n",
    "                cv2.putText(frame, dominant_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        if show_age:\n",
    "            # Alter schätzen\n",
    "            age = int(w / 10)  # Einfache Schätzung basierend auf Gesichtsbreite\n",
    "            cv2.putText(frame, f\"AGE: {age}\", (x, y + h + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        else:\n",
    "            # Nur den grünen Kasten anzeigen\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # Livestream anzeigen\n",
    "    cv2.imshow('Emotion and Age Detection', frame)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "    # Tastenabfrage\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or key == ord('Q'):  # 'q' drücken, um die Schleife zu beenden\n",
    "        break\n",
    "    elif key == ord('e') or key == ord('E'):  # 'e' drücken, um die Emotionen ein- oder auszublenden\n",
    "        show_emotions = not show_emotions\n",
    "    elif key == ord('a') or key == ord('A'):  # 'a' drücken, um das Alter ein- oder auszublenden\n",
    "        show_age = not show_age"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Kameraressourcen freigeben\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

